{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N79HTMsLGnQ"
      },
      "source": [
        "Loading the data for processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XNp8vd7Uadk"
      },
      "outputs": [],
      "source": [
        "# Reference : Blackboard, Subject : Deep Learning ,Section : Assessment, Module : Assignment 1, File name : LoadDataset.ipynb \n",
        "#by Michael Madden\n",
        "# Package imports\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Display plots inline and change default figure size\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZfK70CM16xU"
      },
      "outputs": [],
      "source": [
        "# Reference : Blackboard, Subject : Deep Learning ,Section : Assessment, Module : Assignment 1, File name : LoadDataset.ipynb \n",
        "#by Michael Madden\n",
        "\n",
        "# Use pandas to read the CSV file as a dataframe\n",
        "df = pd.read_csv(\"/content/blobs400.csv\")\n",
        "\n",
        "# The y values are those labelled 'Class': extract their values\n",
        "y = df['Class'].values\n",
        "\n",
        "# The x values are all other columns\n",
        "del df['Class']   # drop the 'Class' column from the dataframe\n",
        "X = df.values     # convert the remaining columns to a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5defFuVR19Ia",
        "outputId": "16f79c51-9c56-4353-e7fe-61d15e80390c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(X): 400\n",
            "len(X[0]): 3\n",
            "len(X[:,0]): 400\n",
            "X: nsamples = 400 , nattribs = 3\n",
            "len(y) 400\n",
            "np.shape(y): (400,)\n",
            "np.shape(yt): (400, 1)\n",
            "y transpose: nsamples = 400 , nattribs = 1\n"
          ]
        }
      ],
      "source": [
        "# Reference : Blackboard, Subject : Deep Learning ,Section : Assessment, Module : Assignment 1, File name : LoadDataset.ipynb \n",
        "#by Michael Madden\n",
        "# Some examples of working with the data, to look at rows/columns\n",
        "print (\"len(X):\", len(X))            # outer array: one per sample\n",
        "print (\"len(X[0]):\", len(X[0]))      # each inner array is the attributes of one sample\n",
        "print (\"len(X[:,0]):\", len(X[:,0]))  # select column 0 from array\n",
        "\n",
        "# np.shape returns all dimensions of the array\n",
        "(nsamples, nattribs) = np.shape(X)\n",
        "print (\"X: nsamples =\", nsamples, \", nattribs =\", nattribs)\n",
        "\n",
        "# Now example the y vector (1D array)\n",
        "print (\"len(y)\", len(y))\n",
        "print (\"np.shape(y):\", np.shape(y))\n",
        "\n",
        "# You can transpose the y data using 'reshape'\n",
        "yt = np.reshape(y, (len(y),1))  \n",
        "print (\"np.shape(yt):\", np.shape(yt))\n",
        "(nsamples, nattribs) = np.shape(yt)\n",
        "print (\"y transpose: nsamples =\", nsamples, \", nattribs =\", nattribs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPFcsx_N2FO6"
      },
      "outputs": [],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, random_state=0, train_size = .70)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, random_state=0, train_size = .50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYEFgvd8dgIO"
      },
      "source": [
        "#=====================Part 1 ====================\n",
        "Implementation of Logistic Regression using neural network. \n",
        "\n",
        "There is no hidden layer and one output layer with three inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfkjZfEvXfTY"
      },
      "outputs": [],
      "source": [
        "                              ##########Refrences#############\n",
        "# https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch/blob/main/03%20Logistic%20Regression/LogisticRegression.py\n",
        "# https://stackoverflow.com/questions/29241056/how-do-i-use-np-newaxis\n",
        "# https://stackoverflow.com/questions/21752989/numpy-efficiently-avoid-0s-when-taking-logmatrix\n",
        "# https://www.geeksforgeeks.org/python-infinity/\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "class Logistic_Regression():    # Creating Class\n",
        "\n",
        "    def __init__(self, lr=1, epochs=1):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "        self.best_w = None\n",
        "        self.best_b = None\n",
        "        self.loss_lowest = float(\"inf\")\n",
        "\n",
        "    def sigmoid(self,s, th=0.55):  # Threshold value th= 0.55 gives best result of 95.83 on prediction\n",
        "        a = 1/(1+np.exp(-s))\n",
        "        for x in range(len(a)):     # Changing the output to 0  or 1 based on the threshold .\n",
        "            if a[x] <= th:\n",
        "                a[x] = 0\n",
        "            else:\n",
        "                a[x] = 1\n",
        "        return a\n",
        "\n",
        "    def cost_funct(self, X, y, w, b):  # cost funtion to average the loss \n",
        "        m = len(y)\n",
        "        h = self.sigmoid(np.dot(X, w) + b)\n",
        "        h = np.clip(h, 1e-15, 1 - 1e-15)\n",
        "        cost = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "        return cost\n",
        "\n",
        "    def loss_funct(self,y_pred,y_true):  # Loss funtion to calculate the loss between the actual and the predicted values\n",
        "        loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "        return loss\n",
        "\n",
        "    def SGD(self, X, y, w, b):    # Implemented Stocastic Gradient Descent for optimization\n",
        "        m = len(y)\n",
        "        for i in range(m):\n",
        "            h = self.sigmoid(np.dot(X[i], w) + b)    # Activation function for predicting the output\n",
        "            w = w - self.lr * (h - y[i]) * X[i][:, np.newaxis]   # updating the weight \n",
        "            b = b - self.lr * (h - y[i])                         # updating the bias\n",
        "        return w, b\n",
        "\n",
        "    def accuracy_funct(self, y, predictn):        # to calculate the accuracy of the predicted result \n",
        "        correct_result = 0\n",
        "        for i in range(len(y)):\n",
        "            if y[i] == predictn[i]:\n",
        "                correct_result += 1\n",
        "        accuracy = (correct_result / len(y)) * 100\n",
        "        return accuracy\n",
        "\n",
        "    def fit(self, X, y):      # Fit function to train the model\n",
        "        self.b = 0\n",
        "        n_samp, n_attr = np.shape(X)\n",
        "        self.w = np.zeros((n_attr, 1))\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.w, self.b = self.SGD(X, y, self.w, self.b)  # calling stochastic gradient descent funtion for optimization and prediction.\n",
        "            cost = self.cost_funct(X, y, self.w, self.b)\n",
        "            loss = self.loss_funct(self.sigmoid(np.dot(X, self.w) + self.b), y)\n",
        "            if cost < self.loss_lowest:\n",
        "                self.loss_lowest = cost\n",
        "                self.best_w = self.w.copy()\n",
        "                self.best_b = self.b\n",
        "\n",
        "    def predict(self, X, y):\n",
        "        pred = np.dot(X, self.best_w) + self.best_b\n",
        "        predictn = self.sigmoid(pred)\n",
        "        accuracy = self.accuracy_funct(y, predictn)\n",
        "        print(\"Accuracy: \", accuracy)\n",
        "        print(\"\\nClassification Report:\\n\", classification_report(y, predictn))\n",
        "        print(\"\\nConfusion matrix:\\n\", confusion_matrix(y, predictn))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkQIbMp5Gl0T"
      },
      "source": [
        "Fit Logistic Regression on Bloobs data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIcyP0jHavp4",
        "outputId": "dd11857e-05da-4cb7-e872-990660d2be5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-26-bba11b364da8>:38: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
            "<ipython-input-26-bba11b364da8>:38: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n"
          ]
        }
      ],
      "source": [
        "logistic_Regg = Logistic_Regression()\n",
        "logistic_Regg.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6uTyJQfGtUW"
      },
      "source": [
        "Validate Logistic Regression on Bloobs data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyCCxFaoGdrI",
        "outputId": "f67efac3-908d-4136-e372-ca1b434d230c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  90.0\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.80      0.89        30\n",
            "           1       0.83      1.00      0.91        30\n",
            "\n",
            "    accuracy                           0.90        60\n",
            "   macro avg       0.92      0.90      0.90        60\n",
            "weighted avg       0.92      0.90      0.90        60\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[24  6]\n",
            " [ 0 30]]\n"
          ]
        }
      ],
      "source": [
        "logistic_Regg.predict(X_val,y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqE2K5IVMwnu"
      },
      "source": [
        "Observation : \n",
        "1) We have 1000 epoch and 0.01 as the learning rate increasing the value of the learning rate is causing the model to run for less numbers of epochs and learns faster. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yymEm0fHG1Ux"
      },
      "source": [
        "Predict Logistic Regression on Bloobs data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYKeyEx7btCW",
        "outputId": "ff190a48-7fea-4e3a-e7f1-90174d32fcfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  91.66666666666666\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.83      0.91        59\n",
            "           1       0.86      1.00      0.92        61\n",
            "\n",
            "    accuracy                           0.92       120\n",
            "   macro avg       0.93      0.92      0.92       120\n",
            "weighted avg       0.93      0.92      0.92       120\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[49 10]\n",
            " [ 0 61]]\n"
          ]
        }
      ],
      "source": [
        "logistic_Regg.predict(X_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHwkb8yHNjMO"
      },
      "source": [
        "#=============Part 2==============="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I49-mrnG9-U"
      },
      "source": [
        "Logistic Regression on Moons data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unM0eSc9tmMG"
      },
      "outputs": [],
      "source": [
        "# Reference : Blackboard, Subject : Deep Learning ,Section : Assessment, Module : Assignment 1, File name : LoadDataset.ipynb \n",
        "#by Michael Madden\n",
        "\n",
        "df1 = pd.read_csv(\"/content/moons500.csv\")\n",
        "y1 = df1['Class'].values\n",
        "del df1['Class'] \n",
        "X1 = df1.values  \n",
        "X1_train, X1_temp, y1_train, y1_temp = train_test_split(X1, y1, random_state=0, train_size = .70)\n",
        "X1_val, X1_test, y1_val, y1_test = train_test_split(X1_temp, y1_temp, random_state=0, train_size = .50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-pHMf-OHGta"
      },
      "source": [
        "Fit Logistic Regression on Moons data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK3jlHcDN6Wc",
        "outputId": "b32bfeca-c2dd-4ec2-8bb4-c221bbd3c8a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-54-aa409c60dd48>:38: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
            "<ipython-input-54-aa409c60dd48>:38: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n"
          ]
        }
      ],
      "source": [
        "logistic_Regg1 = Logistic_Regression()\n",
        "logistic_Regg1.fit(X1_train,y1_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt346OqjHK20"
      },
      "source": [
        "Validate Logistic Regression on Moons data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Anh9Jh6SO8Td",
        "outputId": "7ad6c2a0-ce90-47db-bd25-1af4a994fd00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  85.33333333333334\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.84      0.87        44\n",
            "           1       0.79      0.87      0.83        31\n",
            "\n",
            "    accuracy                           0.85        75\n",
            "   macro avg       0.85      0.86      0.85        75\n",
            "weighted avg       0.86      0.85      0.85        75\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[37  7]\n",
            " [ 4 27]]\n"
          ]
        }
      ],
      "source": [
        "logistic_Regg1.predict(X1_val,y1_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afthWPtSHShA"
      },
      "source": [
        "Predict Logistic Regression on Moons data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dFuVbcMPqi8",
        "outputId": "e5ab8896-999f-4ca1-cffc-4574c221723e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  74.66666666666667\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.64      0.72        39\n",
            "           1       0.69      0.86      0.77        36\n",
            "\n",
            "    accuracy                           0.75        75\n",
            "   macro avg       0.76      0.75      0.75        75\n",
            "weighted avg       0.76      0.75      0.74        75\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[25 14]\n",
            " [ 5 31]]\n"
          ]
        }
      ],
      "source": [
        "logistic_Regg1.predict(X1_test,y1_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFLZoDrzPkQ1"
      },
      "source": [
        "# ===================Part 3========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKwNJbRSjc8W"
      },
      "outputs": [],
      "source": [
        "# Reference : https://vidyasheela.com/post/python-code-to-calculate-the-derivative-of-sigmoid-activation-function\n",
        "#Refrence: https://stackoverflow.com/questions/27516849/how-to-convert-list-of-numpy-arrays-into-single-numpy-array\n",
        "class SNN():\n",
        "    def __init__(self): \n",
        "        self.num_iteration = 1000\n",
        "        self.hidden_nodes = 3\n",
        "        self.num_outputs = 1\n",
        "        self.convergence_threshold = 10*np.exp(-12)\n",
        "        self.loss_lowest=80000\n",
        "        self.lr=0.01\n",
        "\n",
        "    def sigmoid(self,Z):\n",
        "        return (1/(1+np.exp(-Z)))\n",
        "\n",
        "    def for_propagation(self,X_,w1,b1,w2,b2):\n",
        "        self.Z1 = np.dot(X_, w1) + b1\n",
        "        self.a1 = self.sigmoid(self.Z1)\n",
        "        self.Z2 = np.dot(self.a1, w2) + b2\n",
        "        self.y_hat = self.sigmoid(self.Z2)\n",
        "        return self.y_hat, self.Z1, self.a1, self.Z2\n",
        "    \n",
        "    def cost_function(self, y_pred, y_true):\n",
        "        loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "        return loss\n",
        "    \n",
        "    def der_sigmoid(self,x):\n",
        "        derivative_sigmoid = self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "        return derivative_sigmoid\n",
        "  \n",
        "    def accuracy_function(self, y_, predictn, th=0.58):\n",
        "        predictn=np.concatenate( predictn, axis=0 ) # convert to list\n",
        "        predictn = np.where(predictn <= th, 0, 1)  # convert to binary predictions\n",
        "        accuracy = np.mean(y_ == predictn) * 100\n",
        "        return accuracy\n",
        "\n",
        "    def back_propagation(self,y_hat):\n",
        "        self.delta_z2 = y_hat - self.Y_train \n",
        "        self.delta_weights2 = np.dot(self.a1.T, self.delta_z2)\n",
        "        self.delta_bias2 = np.sum(self.delta_z2, axis=0, keepdims=True)\n",
        "        self.delta_Z1 = self.der_sigmoid(self.a1) * np.dot(self.delta_z2, self.w2.T)\n",
        "        self.delta_weight1 = np.dot(self.X.T, self.delta_Z1)\n",
        "        self.delta_bias1 =  np.sum(self.delta_Z1, axis=0, keepdims=True)\n",
        "        self.w1 = self.w1 - self.delta_weight1 * self.lr\n",
        "        self.w2 = self.w2 - self.delta_weights2 * self.lr\n",
        "        self.b1 = self.b1 - self.delta_bias1 * self.lr\n",
        "        self.b2 = self.b2 - self.delta_bias2 * self.lr\n",
        "        return self.w1, self.w2, self.b1, self.b2 \n",
        "\n",
        "    def fit(self,X_train,Y_train):\n",
        "        self.X = X_train\n",
        "        self.num_inputs = self.X.shape[1]\n",
        "        self.w1 = np.random.randn(self.num_inputs, self.hidden_nodes)\n",
        "        self.w2 = np.random.randn(self.hidden_nodes, self.num_outputs)\n",
        "        self.b1 = np.random.randn(1, self.hidden_nodes)\n",
        "        self.b2 = np.random.randn(1, self.num_outputs)\n",
        "        self.Y_train = Y_train.reshape(-1, self.num_outputs)\n",
        "        cost_history = []\n",
        "        self.best_hidden_wt = self.w1.copy()\n",
        "        self.best_hidden_bias = self.b1.copy()\n",
        "        self.best_out_wt = self.w2.copy()\n",
        "        self.best_out_bias = self.b2.copy()\n",
        "\n",
        "        for f in range(self.num_iteration):\n",
        "            y_hat, a1, Z1, Z2 = self.for_propagation(self.X, self.w1, self.b1, self.w2, self.b2)\n",
        "            self.w1, self.w2, self.b1, self.b2 = self.back_propagation(y_hat)\n",
        "            loss = self.cost_function(y_hat,self.Y_train)\n",
        "          # accuracy = self.accuracy_funct(self.Y_train, np.round(y_hat))\n",
        "            cost_history.append(np.mean(loss))\n",
        "           # check for convergence\n",
        "            if f > 0:\n",
        "              if abs(np.mean(loss) - cost_history[f-1]) < self.convergence_threshold:\n",
        "                print(\"Final iteration number: \",f)\n",
        "                self.best_w1 = self.w1.copy()\n",
        "                self.best_b1 = self.b1.copy()\n",
        "                self.best_w2 = self.w2.copy()\n",
        "                self.best_b2 = self.b2.copy()\n",
        "                break\n",
        "                \n",
        "        self.best_w1 = self.w1.copy()\n",
        "        self.best_b1 = self.b1.copy()\n",
        "        self.best_w2 = self.w2.copy()\n",
        "        self.best_b2 = self.b2.copy()\n",
        "\n",
        "\n",
        "    def predict(self,X_):\n",
        "        y_hat, a1,Z1,Z2 = self.for_propagation(X_,self.best_w1,self.best_b1,self.best_w2,self.best_b2)\n",
        "        return y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgsObigUt_ya"
      },
      "source": [
        "Fit Shallow Neural Network on Bloobs data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSb4v-kIPo4N",
        "outputId": "b4eee6f9-01fc-4c7e-89aa-5f4a103479aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final iteration number:  49\n"
          ]
        }
      ],
      "source": [
        "nn = SNN()\n",
        "nn.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAh2vogfH-O1"
      },
      "source": [
        "Validate Shallow Neural Network on Bloobs data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1tM2VWCIeYH",
        "outputId": "3ee870db-b9b1-4184-fcbd-a909b5703a87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 48.333333333333336\n"
          ]
        }
      ],
      "source": [
        "predictnv=nn.predict(X_val)\n",
        "print(\"Accuracy\",nn.accuracy_function(y_test, predictnv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79cx5YyiIdmF"
      },
      "source": [
        "Predict Shallow Neural Network on Bloobs data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y37FqbwBSjNd",
        "outputId": "f015750c-1c6e-470d-83ff-220fa6c6792b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 55.00000000000001\n"
          ]
        }
      ],
      "source": [
        "predictn=nn.predict(X_test)\n",
        "print(\"Accuracy\",nn.accuracy_function(y_test, predictn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxt2aOucuFPC"
      },
      "source": [
        "Fit Shallow Neural Network on Moons data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wWdjAJ4EPDg",
        "outputId": "fc70b009-cde2-49a3-eb16-11e20fbe2a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final iteration number:  53\n"
          ]
        }
      ],
      "source": [
        "nn1 = SNN()\n",
        "nn1.fit(X1_train,y1_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "733DCC8uI1Ew"
      },
      "source": [
        "Validate Shallow Neural Network on Moons data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZapIBJXQIz6A",
        "outputId": "eac22bf0-c3e0-4805-e5a4-49fbb0ccba31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 88.0\n"
          ]
        }
      ],
      "source": [
        "predictnv=nn1.predict(X1_val)\n",
        "print(\"Accuracy\",nn1.accuracy_function(y1_val, predictnv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI2Lye-HI3uJ"
      },
      "source": [
        "Test Shallow Neural Network on Moons data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wablZfMDM_PP",
        "outputId": "16ddbde9-c548-4541-9a56-fbd375a51aba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 85.33333333333334\n"
          ]
        }
      ],
      "source": [
        "predictn1=nn1.predict(X1_test)\n",
        "print(\"Accuracy\",nn1.accuracy_function(y1_test, predictn1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6foWYUHuOGi"
      },
      "source": [
        "# ==============Part 4====================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2TP_UXDuPS6"
      },
      "outputs": [],
      "source": [
        "# Reference : Blackboard, Subject : Deep Learning ,Section : Assessment, Module : Assignment 1, File name : LoadDataset.ipynb \n",
        "#by Michael Madden\n",
        "\n",
        "# This function taken directly from the Fashion-MNIST github site: \n",
        "# https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_mnist(path, kind='train'): \n",
        "    import os\n",
        "    import gzip\n",
        "    import numpy as np\n",
        "\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path,\n",
        "                               '%s-labels-idx1-ubyte.gz'\n",
        "                               % kind)\n",
        "    images_path = os.path.join(path,\n",
        "                               '%s-images-idx3-ubyte.gz'\n",
        "                               % kind)\n",
        "\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784)\n",
        "\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHtl9Iyh24Sn"
      },
      "outputs": [],
      "source": [
        "#Refrence- https://stackoverflow.com/questions/53991131/how-to-split-data-frame-into-x-and-y\n",
        "image,label=load_mnist('/content/')\n",
        "i=pd.DataFrame(image)\n",
        "l= pd.DataFrame(label)\n",
        "i['label'] = l\n",
        "df = i.loc[(i[\"label\"] == 0) | (i[\"label\"] == 1)]\n",
        "df_X = df.iloc[:, 0:-1]\n",
        "df_y = df.iloc[:, -1]\n",
        "# df_y=df[[\"label\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg0sDdE9xQ4J"
      },
      "source": [
        "Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyMBGSEfv4li",
        "outputId": "29201a20-5371-43ac-91ee-6d2640c24855"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-3914cdcdea55>:13: RuntimeWarning: overflow encountered in exp\n",
            "  return (1/(1+np.exp(-Z)))\n",
            "<ipython-input-11-3914cdcdea55>:23: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
            "<ipython-input-11-3914cdcdea55>:23: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n"
          ]
        }
      ],
      "source": [
        "NN11 = SNN()\n",
        "NN11.fit(df_X,df_y.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h65Br5B_xXH4"
      },
      "source": [
        "Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08wQ2cuoxhSo"
      },
      "outputs": [],
      "source": [
        "image,label=load_mnist('/content/',kind='t10k')\n",
        "i=pd.DataFrame(image)\n",
        "l= pd.DataFrame(label)\n",
        "i['label'] = l\n",
        "df = i.loc[(i[\"label\"] == 0) | (i[\"label\"] == 1)]\n",
        "df_X_test = df.iloc[:, 0:-1]\n",
        "df_y_test = df.iloc[:, -1]\n",
        "# df_y=df[[\"label\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prOWxMgjxAMK",
        "outputId": "eb8e6b9f-b6eb-4e6f-b3b4-d0da583ed328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 96.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-3914cdcdea55>:13: RuntimeWarning: overflow encountered in exp\n",
            "  return (1/(1+np.exp(-Z)))\n"
          ]
        }
      ],
      "source": [
        "predictn11=NN11.predict(df_X_test)\n",
        "predictn11 = np.array([np.round(x) for x in predictn11])\n",
        "print(\"Accuracy\",nn1.accuracy_function(df_y_test.values, predictn11))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK8BUAT9JsM8"
      },
      "source": [
        "Adding one extra hidden layer as an enhancement for the Shallow Neural Network "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jW08AaDxyTJ"
      },
      "outputs": [],
      "source": [
        "# Refrence- Refrenced the code from Part 4 of this assignment which was worked by both Yamini and I and made enhancements to it.\n",
        "# Reference : https://vidyasheela.com/post/python-code-to-calculate-the-derivative-of-sigmoid-activation-function\n",
        "\n",
        "class P_SNN():\n",
        "    def __init__(self): \n",
        "        self.num_iteration = 1000\n",
        "        self.hidden_nodes_1 = 3\n",
        "        self.hidden_nodes_2 = 5  # new layer\n",
        "        self.num_outputs = 1\n",
        "        self.convergence_threshold = 10*np.exp(-11)\n",
        "        self.loss_lowest=80000\n",
        "        self.lr=0.001\n",
        "\n",
        "    def sigmoid(self,Z):\n",
        "        return (1/(1+np.exp(-Z)))\n",
        "\n",
        "    def for_propagation(self,X_,w1,b1,w2,b2,w3,b3):  \n",
        "        self.Z1 = np.dot(X_, w1) + b1\n",
        "        self.a1 = self.sigmoid(self.Z1)\n",
        "        self.Z2 = np.dot(self.a1, w2) + b2\n",
        "        self.a2 = self.sigmoid(self.Z2)  # new layer\n",
        "        self.Z3 = np.dot(self.a2, w3) + b3  # new layer\n",
        "        self.y_hat = self.sigmoid(self.Z3)\n",
        "        return self.y_hat, self.Z1, self.a1, self.Z2, self.a2, self.Z3  #  new layer\n",
        "\n",
        "    \n",
        "    def cost_function(self, y_pred, y_true):\n",
        "        loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "        return loss\n",
        "    \n",
        "    def der_sigmoid(self,x):\n",
        "        derivative_sigmoid = self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "        return derivative_sigmoid\n",
        "  \n",
        "    def accuracy_function(self, y_, predictn, th=0.55):\n",
        "        predictn=np.concatenate( predictn, axis=0 )\n",
        "        predictn = np.where(predictn <= th, 0, 1)  \n",
        "        accuracy = np.mean(y_ == predictn) * 100\n",
        "        return accuracy\n",
        "\n",
        "    def back_propagation(self,y_hat):\n",
        "        self.delta_z3 = y_hat - self.Y_train\n",
        "        self.delta_weights3 = np.dot(self.a2.T, self.delta_z3)  # new layer\n",
        "        self.delta_bias3 = np.sum(self.delta_z3, axis=0, keepdims=True)  # new layer\n",
        "        self.delta_z2 = np.dot(self.delta_z3, self.w3.T) * self.der_sigmoid(self.Z2)  # new layer\n",
        "        self.delta_weights2 = np.dot(self.a1.T, self.delta_z2)\n",
        "        self.delta_bias2 = np.sum(self.delta_z2, axis=0, keepdims=True)\n",
        "        self.delta_Z1 = self.der_sigmoid(self.a1) * np.dot(self.delta_z2, self.w2.T)\n",
        "        self.delta_weight1 = np.dot(self.X.T, self.delta_Z1)\n",
        "        self.delta_bias1 =  np.sum(self.delta_Z1, axis=0, keepdims=True)\n",
        "        self.w1 = self.w1 - self.delta_weight1 * self.lr\n",
        "        self.w2 = self.w2 - self.delta_weights2 * self.lr\n",
        "        self.w3 = self.w3 - self.delta_weights3 * self.lr  # new layer\n",
        "        self.b1 = self.b1 - self.delta_bias1 * self.lr\n",
        "        self.b2 = self.b2 - self.delta_bias2 * self.lr\n",
        "        self.b3 = self.b3 - self.delta_bias3 * self.lr  # new layer\n",
        "        return self.w1, self.w2, self.w3, self.b1, self.b2, self.b3  #  new layer\n",
        "\n",
        "\n",
        "    def fit(self,X_train,Y_train):\n",
        "        self.X = X_train\n",
        "        self.num_inputs = self.X.shape[1]\n",
        "        self.w1 = np.random.randn(self.num_inputs, self.hidden_nodes_1)\n",
        "        self.w2 = np.random.randn(self.hidden_nodes_1, self.hidden_nodes_2)  # new layer\n",
        "        self.w3 = np.random.randn(self.hidden_nodes_2, self.num_outputs)   # new layer\n",
        "        self.b1 = np.random.randn(1, self.hidden_nodes_1)\n",
        "        self.b2 = np.random.randn(1, self.hidden_nodes_2)  # new layer\n",
        "        self.b3 = np.random.randn(1, self.num_outputs)  # new layer\n",
        "        self.Y_train = Y_train.reshape(-1, self.num_outputs)\n",
        "        cost_history = []\n",
        "        self.best_hidden_wt = self.w1.copy()\n",
        "        self.best_hidden_bias = self.b1.copy()\n",
        "        self.best_out_wt = self.w2.copy()\n",
        "        self.best_out_bias = self.b2.copy()\n",
        "        self.best_out_wt = self.w3.copy()\n",
        "        self.best_out_bias = self.b3.copy()\n",
        "\n",
        "        for f in range(self.num_iteration):\n",
        "            y_hat, Z1, a1, Z2, a2, Z3 = self.for_propagation(self.X, self.w1, self.b1, self.w2, self.b2, self.w3, self.b3)  # new layer\n",
        "            self.w1, self.w2, self.w3, self.b1, self.b2, self.b3 = self.back_propagation(y_hat)  # new layer\n",
        "            loss = self.cost_function(y_hat,self.Y_train)\n",
        "          # accuracy = self.accuracy_funct(self.Y_train, np.round(y_hat))\n",
        "            cost_history.append(np.mean(loss))\n",
        "           # check for convergence\n",
        "            if f > 0:\n",
        "              if abs(np.mean(loss) - cost_history[f-1]) < self.convergence_threshold:\n",
        "                print(\"Final iteration number: \",f)\n",
        "                self.best_w1 = self.w1.copy()\n",
        "                self.best_b1 = self.b1.copy()\n",
        "                self.best_w2 = self.w2.copy()\n",
        "                self.best_b2 = self.b2.copy()\n",
        "                self.best_w3 = self.w3.copy()   # new layer\n",
        "                self.best_b3 = self.b3.copy()   # new layer\n",
        "                break\n",
        "                \n",
        "        self.best_w1 = self.w1.copy()\n",
        "        self.best_b1 = self.b1.copy()\n",
        "        self.best_w2 = self.w2.copy()\n",
        "        self.best_b2 = self.b2.copy()\n",
        "        self.best_w3 = self.w3.copy()    # new layer\n",
        "        self.best_b3 = self.b3.copy()    # new layer\n",
        "\n",
        "\n",
        "    def predict(self,X_):\n",
        "        y_hat, Z1, a1, Z2, a2, Z3 = self.for_propagation(X_,self.best_w1,self.best_b1,self.best_w2,self.best_b2,self.best_w3,self.best_b3)  # new layer\n",
        "        return y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uosik4MvJ0g9"
      },
      "source": [
        "Fit Enhanced Shallow Neural Network on Moons Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEv0f68fVOsM",
        "outputId": "2818c29a-7397-4f4c-c3e4-e6672253530f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final iteration number:  381\n"
          ]
        }
      ],
      "source": [
        "P_nn = P_SNN()\n",
        "P_nn.fit(X1_train,y1_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH8jZx2eKNMg"
      },
      "source": [
        "Validate Enhanced Shallow Neural Network on Moons Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXm8jr6NKaQd",
        "outputId": "8c7d9086-0f95-4990-a35f-4707fcd323df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 88.0\n"
          ]
        }
      ],
      "source": [
        "predictn=P_nn.predict(X1_val)\n",
        "print(\"Accuracy\",nn.accuracy_function(y1_val, predictn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm-68BsxKVbt"
      },
      "source": [
        "Predict Enhanced Shallow Neural Network on Moons Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY-PQnf-Wggc",
        "outputId": "9582e37b-d723-4859-8b89-2c274945e260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 81.33333333333333\n"
          ]
        }
      ],
      "source": [
        "predictn=P_nn.predict(X1_test)\n",
        "print(\"Accuracy\",nn.accuracy_function(y1_test, predictn))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
